# RAG Financial AI Agent Configuration
# Copy this file to .env.local and customize as needed

# ==========================================
# Ollama Configuration
# ==========================================
# Ollama API endpoint (local installation)
OLLAMA_BASE_URL=http://localhost:11434

# Language model for chat responses
# Options: llama3, llama3:8b, llama3:70b, codellama, mistral, etc.
OLLAMA_MODEL=llama3

# Embedding model for document vectorization
# Options: nomic-embed-text, mxbai-embed-large, etc.
OLLAMA_EMBED_MODEL=nomic-embed-text

# ==========================================
# Service Configuration
# ==========================================
# Python FastAPI backend URL
PY_BACKEND_URL=http://localhost:8000

# Python backend host (0.0.0.0 for external access, 127.0.0.1 for local only)
PY_HOST=0.0.0.0

# Python backend port
PY_PORT=8000

# Next.js frontend port (optional, defaults to 3000)
# PORT=3000

# ==========================================
# Document Processing
# ==========================================
# Text chunk size in tokens for document splitting
CHUNK_SIZE_TOKENS=512

# Overlap between chunks to preserve context
CHUNK_OVERLAP_TOKENS=64

# ==========================================
# Storage Configuration
# ==========================================
# Root directory for all storage
STORAGE_ROOT=storage

# Vector index storage directory
INDEX_DIR=storage/index

# Uploaded files directory
UPLOAD_DIR=storage/uploads

# ==========================================
# Retrieval Configuration
# ==========================================
# Default number of similar chunks to retrieve
SIMILARITY_TOP_K=5

# Enable sentence transformer reranking (requires torch & sentence-transformers)
# Improves result quality but increases processing time
ENABLE_RERANK=false

# Reranking model (if ENABLE_RERANK=true)
RERANK_MODEL=BAAI/bge-reranker-base

# Number of top results after reranking
RERANK_TOP_N=2

# Enable LLM-based reranking (uses main LLM for result scoring)
# Provides highest quality but slowest processing
ENABLE_LLM_RERANK=false

# Number of top results after LLM reranking
LLM_RERANK_TOP_N=2

# Sentence window size for context preservation
SENTENCE_WINDOW_SIZE=3

# ==========================================
# Development & Debugging
# ==========================================
# Enable debug logging (true/false)
# DEBUG=false

# Log level (DEBUG, INFO, WARNING, ERROR)
# LOG_LEVEL=INFO

# ==========================================
# Production Configuration
# ==========================================
# For production deployment, consider:
# - Setting OLLAMA_BASE_URL to your Ollama server
# - Configuring proper CORS origins instead of "*"
# - Setting up SSL/TLS certificates
# - Implementing authentication
# - Adding monitoring and logging aggregation

# Example production overrides:
# OLLAMA_BASE_URL=https://your-ollama-server:11434
# PY_HOST=127.0.0.1  # More restrictive for production
# ENABLE_RERANK=true  # Better quality for production
# SIMILARITY_TOP_K=10  # More comprehensive retrieval
