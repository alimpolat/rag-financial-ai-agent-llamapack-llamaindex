# Ollama configuration (local LLaMA runtime)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3
OLLAMA_EMBED_MODEL=nomic-embed-text

# Chunking parameters (tokens)
CHUNK_SIZE_TOKENS=512
CHUNK_OVERLAP_TOKENS=64

# Python backend (FastAPI) proxy URL
PY_BACKEND_URL=http://localhost:8000

# Retrieval config
SIMILARITY_TOP_K=6
ENABLE_RERANK=false
RERANK_MODEL=BAAI/bge-reranker-base
RERANK_TOP_N=2
ENABLE_LLM_RERANK=false
LLM_RERANK_TOP_N=2
SENTENCE_WINDOW_SIZE=3
